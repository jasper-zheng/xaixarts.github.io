---
layout: default
---

<!-- <center>
<h1 style="font-weight: bold;">About</h1>
</center> -->


# **About**
This first ACM C&C workshop on explainable AI for the Arts (XAIxArts) brings together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. XAI is a core concern of Human-Centred AI and relies heavily on Human-Computer Interaction techniques to explore how complex and difficult to understand AI models such as deep learning techniques can be made more understandable to people. However, to date XAI research has primarily focused on work-oriented and task-oriented explanations of AI and there has been little research on XAI for creative domains such as the arts. 
The workshop will take place online and will question the nature, meaning, and role of the explainability of AI for the Arts. The goals of this online workshop are to:
* Build an interdisciplinary XAIxArts research community and network;
* Map out the current and future possible landscapes of XAIxArts;
* Critically reflect on the potential of XAI for the Arts, forming the basis for future collaborations and publications;
* Develop ideas for an outline proposal for an edited book on XAIxArts.

**Workshop day (online): Mon 19th June 2023**

## **Themes**

The themes include <u>but are not limited to</u>:
### The Nature of Explanations
There are open questions about how much explanation is needed or even useful in XAIxArts. What does it mean to actually understand an AI in a creative context i.e. how much explanation is useful versus the risk of over-explaining, and how can we design for both explanation *and* serendipitous interaction? How do we strike a balance between explanation, exploration, and surprise? Being surprised, confused, and reflective are often integral to creative practice but are in opposition to the more functional goals of XAI.

### AI Models, Features, and Training Data
There are many generative AI models which create content across the arts from visual arts to music. However, there is very little research on how these AI models are actually used and appropriated in creative practice. There are open questions about: What kinds of AI models are most amenable to XAIxArts for different artistic domains such as music or visual arts; Which XAI models are more amenable for different artistic processes; Which features of AI models offer the most useful explanations for different creative practices; and What kinds of training sets are more or less useful for creating XAIxArts? 

### User Centred Design of XAIxArts
A fundamental concern of XAI research is how AI can be made more understandable to humans. There is little research on how User Centred Design (UCD) can be used in the creation of XAI systems, and no current research on UCD for XAIxArts. There are open questions about how artists and creative practitioners can be involved in the co-design of generative AI systems involving data scientists and interaction designers, for example, how to include musicians in the design of XAI generative music systems. 

### Interaction Design
As XAIxArts is a nascent research area with no XAI models currently being used in practice there is little research on how to design the interaction with XAI for creative practice. This leaves open Interaction Design questions including: Which features of the AI model to expose to the user; How to visualise highly multi-dimensional data typical of AI models e.g. How to visualise latent spaces with 4 or more semantic dimensions; How to navigate and explore the spaces inside AI models; How to handle the entanglement between dimensions within a latent space; How to manage interaction with XAI in combination with the temporal and affective dimensions of artistic practice. 

# **Accepted Papers**

**Luís Arandas, Mick Grierson and Miguel Carvalhais** Antagonising explanation and revealing bias directly through sequencing and multimodal inference [[paper]](https://xaixarts.github.io/accepted-2023/Arandas-XAIxArts-2023-Paper.pdf) 

**Cheshta Arora and Debarun Sarkar** The Injunction of XAIxArt: Moving beyond explanation to sense-making [[paper]](https://xaixarts.github.io/accepted-2023/Arora-XAIxArts-2023-Paper.pdf)

**Marianne Bossema, Rob Saunders and Somaya Ben Allouch** Human-Machine Co-Creativity with Older Adults -- A Learning Community to Study Explainable Dialogues [[paper]](https://xaixarts.github.io/accepted-2023/Bossema-XAIxArts-2023-Paper.pdf)

**Michael Clemens** Explaining the Arts: Toward a Framework for Matching Creative Tasks with Appropriate Explanation Mediums [[paper]](https://xaixarts.github.io/accepted-2023/Clemens-XAIxArts-2023-Paper.pdf) 

**Drew Hemment, Matjaz Vidmar, Daga Panas, Dave Murray-Rust, Vaishak Belle and Ruth Aylett** Agency and legibility for artists through Experiential AI [[paper]](https://xaixarts.github.io/accepted-2023/Hemment-XAIxArts-2023-Paper.pdf)

**Petra Jääskeläinen** Explainable Sustainability for AI in the Arts [[paper]](https://xaixarts.github.io/accepted-2023/Jääskeläinen-XAIxArts-2023-Paper.pdf) 

**Jamal Knight, Andrew Johnston and Adam Berry** Artistic control over the glitch in AI-generated motion capture [[paper]](https://xaixarts.github.io/accepted-2023/Knight-XAIxArts-2023-Paper.pdf)

**Makayla Lewis** AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block [[paper]](https://xaixarts.github.io/accepted-2023/Lewis-XAIxArts-2023-Paper.pdf) 

**Ashley Noel-Hirst and Nick Bryan-Kinns** An Autoethnographic Exploration of XAI in Algorithmic Composition [[paper]](https://xaixarts.github.io/accepted-2023/Noel-Hirst-XAIxArts-2023-Paper.pdf)

**Nicola Privato and Jack Armitage** A Context-Sensitive Approach to XAI in Music Performance [[paper]](https://xaixarts.github.io/accepted-2023/Privato-XAIxArts-2023-Paper.pdf) 

**Gabriel Vigliensoni and Rebecca Fiebrink** Interacting with neural audio synthesis models through interactive machine learning [[paper]](https://xaixarts.github.io/accepted-2023/Vigliensoni-XAIxArts-2023-Paper.pdf)

**Lanxi Xiao, Weikai Yang, Haoze Wang, Shixia Liu and Qiong Wu** Why AI Fails: Parallax [[paper]](https://xaixarts.github.io/accepted-2023/Xiao-XAIxArts-2023-Parallax-Paper.pdf) 

**Lanxi Xiao, Weikai Yang, Haoze Wang, Shixia Liu and Qiong Wu** Why AI Fails: Shortcut

**Hanjie Yu, Yan Dong and Qiong Wu** User-centric AIGC products: Explainable Artificial Intelligence and AIGC products [[paper]](https://xaixarts.github.io/accepted-2023/Yu-XAIxArts-2023-Paper.pdf) 

 
<!-- # **Call for Participation**

To participate in the workshop please submit either A) a position paper, B) a short video, or C) a pictorial. Your submission should tell us about your XAI and/or Arts research and practice addressing the themes and open questions on the workshop website. Submission requirements are:

* **A) Position paper**\
Papers should be submitted to [the EasyChair submission system](https://easychair.org/conferences/?conf=xaixarts2023) as a PDF in the ACM SIGCHI submission template format [(SIGCHI ACM new, standardized single-column format)](https://www.acm.org/publications/proceedings-template) and a maximum of 3 pages in length. Note that Microsoft Word users should use the interim template. Papers do not need to be anonymised (in LaTex use the command \documentclass[manuscript,review]).

* **B) Short video**\
Upload your video (maximum 5 minutes) to a file transfer site such as WeTransfer and submit a PDF document to the [EasyChair submission system](https://easychair.org/conferences/?conf=xaixarts2023) stating that this is a video submission and providing the URL to download your video. Note that videos cannot be uploaded directly to the EasyChair system. Please ensure that the download link is valid until at least 8 May 2023. Please complete all sections of the EasyChair submission system including providing a Title and Abstract to briefly describe the content of your video.

* **C) Pictorial**\
Pictorials should be submitted to [EasyChair](https://easychair.org/conferences/?conf=xaixarts2023) using the C&C 2023 Pictorials template (for [inDesign](https://www.dropbox.com/s/53hzdfb9h8naa6w/ACMCC_2023_Pictorials_inDesign_Template_Folder.zip?dl=0), [Word](https://www.dropbox.com/scl/fi/n4dyx6f0uxhjmreg06crj/ACMCC_2023_Pictorials_WORD_Template.docx?dl=0&rlkey=hzc484kst04ey840e2wt1phd8) or [Powerpoint](https://www.dropbox.com/scl/fi/lvjoa01tefthnji58u7t8/ACMCC_2023_Pictorials_PowerPoint_Template.pptx?dl=0&rlkey=79j6hfipqaxkjukxbsues8u8n)), maximum 5 pages PDF. Maximum file size 50MB. Include the submission's title, author(s) and their affiliation(s), and a 150-word abstract on the first page. In keeping with C&C Pictorial submissions, additional written sections such as Introduction, Conclusion, Discussion, Acknowledgements, and References are *optional*. The submission should focus on an annotated visual composition and use the format creatively. Examples can be found on the [C&C 2023 website](https://cc.acm.org/2023/pictorials/). Please complete all sections of the EasyChair submission system, including a Title and Abstract.  


Participants will be selected based on the quality of their contribution to the debate about XAIxArts with a view to creating a balance of topics in the workshop. Papers and videos will be made available on the workshop website prior to the workshop and copyright is retained by authors. The workshop will take place online with the option for hybrid participation where appropriate. 


<!-- Accepted participants will need to register for our workshop via the ACM Creativity and Cognition 2023 conference website. -->

<!-- Please note that at least one author of each accepted position paper must attend the workshop and that all participants must register for both the workshop and the ACM Creativity and Cognition 2023 conference via the conference website.

Important information: 
* Submissions due: Tue 2nd May 2023 (deadline extended to allow for video and pictorial submissions)
* Participants notified of acceptance: Mon 8th May 2023
* Workshop day (online): Mon 19th June 2023 -->

# **Organizers**
* Nick Bryan-Kinns, Queen Mary University of London, United Kingdom
* Corey Ford, Queen Mary University of London, United Kingdom
* Alan Chamberlain, University of Nottingham, United Kingdom
* Steve Benford, University of Nottingham, United Kingdom
* Helen Kennedy, University of Nottingham, United Kingdom
<!-- * Sid Fels, University of British Columbia, Canada -->
* Zijin Li, Central Conservatory of Music, China
* Wu Qiong, Tsinghua University, China
* Gus Xia, NYU Shanghai, China
* Jeba Rezwana, University of North Carolina at Charlotte, USA

# **Acknowledgements**
We would like to acknowledge the support of the Engineering and Physical Sciences Research Council [grant number EP/S035362/1] PETRAS 2 & [grant number EP/V00784X/1] UKRI Trustworthy Autonomous Systems Hub. We also acknowledge support from the UKRI Centre for Doctoral Training in Artificial Intelligence and Music, supported by UK Research and Innovation [grant number EP/S022694/1].

![image](PETRAS logo article.jpg){:width="28%"} ![image](tas_logo.png){:width="40%"} ![image](AIM LOGO.png){:width="27%"}




# **Contacts**
If you have any questions feel free to contact Nick or Corey at the following email addresses:
- n.bryan-kinns@qmul.ac.uk
- c.j.ford@qmul.ac.uk